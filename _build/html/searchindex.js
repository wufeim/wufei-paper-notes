Search.setIndex({docnames:["areas/transformers","index","transformers/rethinking_positional_encoding_in_language_pretraining"],envversion:{"sphinx.domains.c":2,"sphinx.domains.changeset":1,"sphinx.domains.citation":1,"sphinx.domains.cpp":4,"sphinx.domains.index":1,"sphinx.domains.javascript":2,"sphinx.domains.math":2,"sphinx.domains.python":3,"sphinx.domains.rst":2,"sphinx.domains.std":2,sphinx:56},filenames:["areas/transformers.rst","index.rst","transformers/rethinking_positional_encoding_in_language_pretraining.rst"],objects:{},objnames:{},objtypes:{},terms:{"class":2,The:2,add:2,affili:2,attent:2,author:2,bert:2,between:2,bias:2,bring:2,cl:2,correl:2,di:2,dot:2,e:2,embed:2,encod:[0,1],g:2,guolin:2,he:2,heterogen:2,includ:2,index:[],inform:2,involv:2,ke:2,languag:[0,1],liu:2,microsoft:2,mix:2,modul:[],nearbi:2,page:[],posit:[0,1],pre:[0,1],product:2,research:2,resourc:2,rethink:[0,1],search:[],term:2,tie:2,token:2,toward:2,train:[0,1],transform:[1,2],two:2,unwant:2,usual:2,we:2,which:2,word:2,yan:2},titles:["Transformers","Paper Reading Notes","Rethinking Positional Encoding in Language Pre-training"],titleterms:{"2022":1,By:1,area:1,aug:1,content:[],document:[],encod:2,idea:2,indic:[],kei:2,languag:2,note:1,paper:1,posit:2,pre:2,read:1,rethink:2,s:[],tabl:[],train:2,transform:0,welcom:[],wufei:[]}})