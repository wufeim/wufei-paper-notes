Search.setIndex({docnames:["aug_2022","by_areas","compositional/amodal_seg_bayesian_model","compositional/compositional_cnn","compositional/index","index","nerf/index","nerf/pixelnerf","transformers/index","transformers/paper1","transformers/paper2","transformers/rethinking_positional_encoding_in_language_pretraining","vqa/calibrating_concepts_and_operations","vqa/index"],envversion:{"sphinx.domains.c":2,"sphinx.domains.changeset":1,"sphinx.domains.citation":1,"sphinx.domains.cpp":4,"sphinx.domains.index":1,"sphinx.domains.javascript":2,"sphinx.domains.math":2,"sphinx.domains.python":3,"sphinx.domains.rst":2,"sphinx.domains.std":2,sphinx:56},filenames:["aug_2022.rst","by_areas.rst","compositional/amodal_seg_bayesian_model.rst","compositional/compositional_cnn.rst","compositional/index.rst","index.rst","nerf/index.rst","nerf/pixelnerf.rst","transformers/index.rst","transformers/paper1.rst","transformers/paper2.rst","transformers/rethinking_positional_encoding_in_language_pretraining.rst","vqa/calibrating_concepts_and_operations.rst","vqa/index.rst"],objects:{},objnames:{},objtypes:{},terms:{"0":[2,3,12],"1":[2,3,7,8,11,12],"110m":11,"15595":[],"2":[3,7,8,11,12],"2006":[],"2019":11,"2020":3,"2021":[2,7,11,12],"2022":[2,5],"2d":[2,3,7],"3":[3,7,11,12],"3d":7,"4":[3,11,12],"47":12,"5":11,"6":11,"9":12,"98":12,"class":[2,3,9,10,11],"do":[3,11],"final":[2,7],"function":3,"import":12,"long":12,"new":[11,12],"return":7,"true":3,A:[0,2,4,5,7,11,12],By:5,For:[7,12],In:[2,3,7,11,12],It:[11,12],One:[0,5,6],The:[2,3,7,9,10,11,12],Then:7,There:12,To:7,_:3,_y:3,a_:[],ab:11,ablat:11,absolut:11,accuraci:12,achiev:12,acl:11,across:11,activ:3,acycl:3,ad:[7,11],adam:[2,3],add:[9,10,11],addit:[2,11],affili:[2,3,7,9,10,11,12],aggreg:7,agnost:7,aim:2,alan:[2,3,12],alex:7,align:[3,7],all:11,allow:7,alpha_:[3,11],also:2,altern:2,amod:[0,4,5],an:[2,3,7,11,12],analysi:11,angjoo:7,annot:2,appli:2,approach:[2,7],ar:[2,3,7,11,12],arbitrari:7,architectur:[0,4,5,11],area:5,argu:11,art:7,artifici:3,arxiv:[3,11,12],associ:7,attent:[9,10,11],aug:5,augment:3,author:[2,3,7,9,10,11,12],averag:7,b:12,b_a:2,background:2,backpropag:3,balanc:12,bar:2,base:12,baselin:7,bayesian:[0,4,5],been:3,begin:[3,7],below:[7,11],benchmark:11,benjamin:12,berkelei:7,bert:[9,10,11],beta:3,between:[3,9,10,11,12],bewteen:[11,12],bi:12,bia:11,bias:[9,10,11],bilinear:7,block:7,boost:12,both:7,bound:2,boundari:2,box:2,bring:[9,10,11],c:[7,11,12],c_:12,calcul:11,calibr:[0,5,7,13],call:11,camera:7,can:[3,7,12],captur:[7,12],categori:[2,7],cco:12,characterist:12,cihang:12,cl:[9,10,11],clark:11,classif:3,classifi:3,clevr:12,cnn:3,coco:3,coeffici:3,color:7,com:3,comparison:12,complex:11,compos:3,composit:[0,1,5],compositionalnet:3,comput:[3,7],concentr:11,concept:[0,5,13],condit:[2,7],consid:11,constant:3,continu:7,convolut:[0,4,5],correl:[9,10,11,12],cross:3,current:7,cvpr:[2,3,7],d:[2,3,7,11],data:[2,3,12],dataset:[7,12],dcnn:3,deep:[0,4,5],demonstr:11,densiti:7,depend:11,depict:7,descript:12,detail:[],dev:[11,12],develop:11,di:[9,10,11],differ:[3,12],differenti:[3,7],direct:[3,7,12],distribut:[0,3,4,5,11,12],dnn:3,doe:11,dot:[3,7,9,10,11],ds:7,dt:7,dtu:7,due:12,dure:3,durm:12,e:[3,7,9,10,11,12],each:[3,7,11,12],effect:11,effici:11,either:3,elia:12,embed:[9,10,11,12],enabl:12,encod:[0,5,7,8,9,10,12],end:[3,7],entropi:3,eskin:12,eta:2,even:[2,3],everi:[3,7],exactli:11,exist:7,exp:7,experi:[7,11,12],experiment:11,explain:11,explan:11,explicitli:12,expos:3,extend:7,extract:7,f:[2,3,7],f_1:7,f_2:7,f_a:2,f_p:3,featur:[2,3,7,11],feed:3,few:[0,5,6],field:[0,5,6],figur:[3,7,11,12],find:3,fine:11,first:11,fisher:[2,3],fix:11,focu:11,follow:[11,12],foreground:2,formul:2,forward:3,four:[3,11],frac:[3,11],framework:7,frequenc:12,from:[0,2,5,6,11,12],fulli:3,fuse:11,g:[9,10,11],gamma:7,gamma_1:3,gamma_2:3,gamma_3:3,gener:[2,3,12],give:2,given:[2,7],global:7,glue:11,gong19a:[],gong:11,gqa:12,graph:3,guolin:[9,10,11],h:3,ha:3,handl:7,hat:7,have:[3,11,12],he:[3,9,10,11],head:11,henc:2,heterogen:[9,10,11],hierarch:12,hopkin:[2,3,12],howev:11,html:[],http:[],i:[2,7,11],iccv:12,iclr:11,icml:11,idea:[],identifi:12,ij:11,illustr:11,imag:[0,2,5,6,13],implement:11,improvemnet:12,includ:[9,10,11,12],incorpor:7,increas:11,independ:7,indic:2,infer:3,inferenc:3,inform:[7,9,10,11,12],initi:7,innat:[0,4,5],input:[2,7],instanc:12,instead:11,int_:7,integr:3,intermedi:7,interpol:7,introduc:2,invis:2,invol:7,involv:[9,10,11],its:7,j:3,john:[2,3,12],ju:3,just:11,k:[3,11],kanazawa:7,ke:[9,10,11],kei:[],khandelw:11,kortylewski:[2,3],l:[3,11],label:[2,3],label_by_area:[],labmda:[],lambda:3,lambda_k:3,languag:[0,5,8],larg:2,latent:2,lattic:3,layer:[3,7,11],lead:12,learn:[2,7,11,12],learnabl:11,learnt:2,left:7,let:[2,3,7],level:3,levi:11,li:[11,12],like:11,likelihood:3,limit:12,link:[2,3,7,11,12],liu:[3,9,10,11],local:[3,7,11],look:11,loss:3,lstm:12,lvert:3,m:[2,3,7],magnitud:12,mai:11,major:12,make:11,man:11,mani:[7,11],map:2,margin:[2,3],mathbb:[3,7,11],mathbf:7,mathcal:[2,3],matric:11,matthew:7,maximum:3,method:[2,11,12],microsoft:[9,10,11],mid:[2,3],mise:[2,3],mix:[3,9,10,11],mixtur:[2,3],mlr:[],model:[0,1,3,5,7,12],modul:12,moreov:11,ms:3,mu_k:3,multipl:7,n:7,nearbi:[9,10,11],necessarili:11,nerf:[1,5,7],network:[0,2,4,5,7],neural:[0,2,4,5,6,12],non:2,norm:12,normal:[3,12],note:[],notic:12,novel:7,nscl:12,number:7,o:11,object:[2,3],obstacl:12,obtain:7,occlud:[2,3],occlus:[0,2,4,5,7],often:11,omega:3,one:7,onli:[2,11,12],onto:7,oper:[0,5,7,13],optim:7,org:[],other:11,out:[0,4,5],outperfom:2,outperform:[2,3,7],output:3,over:12,overrightarrow:2,overview:[7,12],p:[2,3,7],p_a:2,p_i:11,p_iw:11,p_j:11,p_jw:11,paper:11,paradigm:12,paramet:[3,11],partial:[0,4,5],particular:3,pascal3d:3,pass:[3,7],perform:12,phi:7,pi:7,pipelin:7,pixelnerf:[0,5,6],plane:7,point:7,pool:7,poorli:12,posit:[0,3,5,8,9,10,12],pre:[0,5,8],predict:[7,12],predictor:12,press:[],prior:2,problem:2,proceed:[],prod_:2,prod_p:3,product:[9,10,11],program:12,progresss:11,project:[2,7,11],propos:[2,3,7,11,12],psi:2,pyramid:7,q:[3,11,12],qin:11,qing:3,quan:12,quantit:7,question:12,r:[3,7,11],radianc:[0,5,6],rai:7,real:[0,3,5,11,13],reason:[0,3,5,13],recent:3,reconstruct:7,redund:[11,12],regular:[3,11],rel:11,relat:11,remov:[11,12],render:7,represent:7,requir:7,research:[9,10,11],residu:7,resnet:7,resourc:[9,10,11],result:[3,7,11],rethink:[0,5,8],retriev:7,rgb:7,right:7,robust:[0,4,5],robustli:3,rvert:3,s:[7,11],same:[2,11],scene:7,score:11,second:11,seem:11,segment:[0,4,5],self:11,sentenc:11,set:11,sever:11,shape:2,shapenet:7,share:11,show:[3,7,11,12],sigma:7,sigma_k:3,significantli:7,singl:[3,7],size:12,some:11,sort:[],space:7,spatial:2,specif:12,split:12,sqrt:11,stack:11,stand:11,standard:[3,7],state:7,stengel:12,step:12,strong:11,strongli:[3,12],studi:11,subsequ:11,substanti:12,sum_:[2,3],sum_k:3,sun:2,supervis:2,symbol:[0,5,13],synthesi:7,synthet:12,t:[3,7,11],t_f:7,t_n:7,tail:12,tak:2,tancik:7,task:[0,4,5],technic:[],term:[3,9,10,11],test:[7,12],text:[3,11,12],textual:11,th:12,thecvf:3,therefor:11,theta:11,theta_i:3,theta_k:3,thi:[2,7,11,12],third:11,through:[0,4,5],thte:7,tie:[9,10,11],time:[2,3,7],token:[9,10,11],top:[3,11],toward:[0,5,9,10,11,13],train:[0,2,3,5,8],tran:12,transform:[1,5,7,9,10,11],treat:11,tupe:11,two:[9,10,11,12],type:12,u:11,uc:7,under:3,underli:12,unequ:12,unifi:3,uniform:11,univers:[2,3,12],unlik:12,unti:11,unwant:[9,10,11],us:[2,3,12],usual:[9,10,11],v97:[],v:7,valu:11,van:12,vari:12,variabl:2,variant:11,vector:[7,11],vehicl:3,veri:11,via:7,vicki:7,view:7,vision:[],visual:[11,12],vmf:3,volum:7,volumetr:7,von:[2,3],vqa:[1,5],vwf:3,w:[2,3,7,11],w_:12,w_a:2,w_i:11,w_iw:11,w_j:11,w_jw:11,wang:11,we:[7,9,10,11],weight:[2,3,12],well:[3,12],what:11,when:[2,3],where:[2,3,7],which:[2,3,9,10,11,12],whole:[11,12],wide:[2,3],within:2,without:2,word:[9,10,11],work:[7,11,12],world:[7,12],x:7,xie:12,y:[2,3,12],yan:[9,10,11],ye:7,yihong:2,yixiao:12,yu:7,yuill:[2,3,12],z:[3,11,12],z_p:3,zeta:2,zhang:12,zhuowan:12},titles:["Aug 2022","By Areas","Amodal Segmentation through Out-of-Task and Out-of-Distribution with a Bayesian Model","Compositional Convolutional Neural Networks: A Deep Architecture with Innate Robustness to Partial Occlusion","Compositional Models","Paper Reading Notes","NeRF","pixelNeRF: Neural Radiance Fields from One or Few Images","Transformers","Paper 1","Paper 2","Rethinking Positional Encoding in Language Pre-training","Calibrating Concepts and Operations: Towards Symbolic Reasoning on Real Images","VQA"],titleterms:{"1":9,"2":10,"2022":0,A:3,By:1,One:7,amod:2,architectur:3,area:1,aug:0,bayesian:2,calibr:12,composit:[3,4],concept:12,content:5,convolut:3,deep:3,detail:[2,3,7,11,12],distribut:2,encod:11,few:7,field:7,from:7,idea:[2,3,7,9,10,11,12],imag:[7,12],innat:3,kei:[2,3,7,9,10,11,12],languag:11,model:[2,4],nerf:6,network:3,neural:[3,7],note:[2,3,5,7,11,12],occlus:3,oper:12,out:2,paper:[4,5,6,8,9,10,13],partial:3,pixelnerf:7,posit:11,pre:11,radianc:7,read:5,real:12,reason:12,refer:[2,3,7,11,12],rethink:11,robust:3,segment:2,summari:[2,3,7,11,12],symbol:12,task:2,technic:[2,3,7,11,12],through:2,toward:12,train:11,transform:8,vision:[],vqa:13}})