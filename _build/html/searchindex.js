Search.setIndex({docnames:["aug_2022","by_areas","compositional/amodal_seg_bayesian_model","compositional/compositional_cnn","compositional/index","index","nerf/index","nerf/pixelnerf","transformers/index","transformers/paper1","transformers/paper2","transformers/rethinking_positional_encoding_in_language_pretraining"],envversion:{"sphinx.domains.c":2,"sphinx.domains.changeset":1,"sphinx.domains.citation":1,"sphinx.domains.cpp":4,"sphinx.domains.index":1,"sphinx.domains.javascript":2,"sphinx.domains.math":2,"sphinx.domains.python":3,"sphinx.domains.rst":2,"sphinx.domains.std":2,sphinx:56},filenames:["aug_2022.rst","by_areas.rst","compositional/amodal_seg_bayesian_model.rst","compositional/compositional_cnn.rst","compositional/index.rst","index.rst","nerf/index.rst","nerf/pixelnerf.rst","transformers/index.rst","transformers/paper1.rst","transformers/paper2.rst","transformers/rethinking_positional_encoding_in_language_pretraining.rst"],objects:{},objnames:{},objtypes:{},terms:{"0":2,"1":[2,3,7,8,11],"110m":11,"15595":[],"2":[7,8,11],"2006":[],"2019":11,"2020":3,"2021":[2,7,11],"2022":[2,5],"2d":[2,7],"3":[7,11],"3d":7,"4":11,"5":11,"6":11,"class":[2,9,10,11],"do":11,"final":[2,7],"new":11,"return":7,A:[0,2,4,5,7,11],By:5,For:7,In:[2,3,7,11],It:11,One:[0,5,6],The:[2,7,9,10,11],Then:7,To:7,_:[],ab:11,ablat:11,absolut:11,acl:11,across:11,ad:[7,11],adam:[2,3],add:[9,10,11],addit:[2,11],affili:[2,3,7,9,10,11],aggreg:7,agnost:7,aim:2,alan:[2,3],alex:7,align:7,all:11,allow:7,alpha_:11,also:2,altern:2,amod:[0,4,5],an:[2,7,11],analysi:11,angjoo:7,annot:2,appli:2,approach:[2,7],ar:[2,7,11],arbitrari:7,architectur:[0,4,5,11],area:5,argu:11,art:7,arxiv:[3,11],associ:7,attent:[9,10,11],aug:5,author:[2,3,7,9,10,11],averag:7,b_a:2,background:2,bar:2,baselin:7,bayesian:[0,4,5],begin:7,below:[7,11],benchmark:11,berkelei:7,bert:[9,10,11],between:[9,10,11],bewteen:11,bia:11,bias:[9,10,11],bilinear:7,block:7,both:7,bound:2,boundari:2,box:2,bring:[9,10,11],c:[7,11],calcul:11,calibr:7,call:11,camera:7,can:7,captur:7,categori:[2,7],cl:[9,10,11],clark:11,color:7,com:3,complex:11,composit:[0,1,5],comput:7,concentr:11,condit:[2,7],consid:11,continu:7,convolut:[0,4,5],correl:[9,10,11],current:7,cvpr:[2,3,7],d:[2,7,11],data:2,dataset:7,deep:[0,4,5],demonstr:11,densiti:7,depend:11,depict:7,detail:[],dev:11,develop:11,di:[9,10,11],differenti:7,direct:7,distribut:[0,4,5,11],doe:11,dot:[7,9,10,11],ds:7,dt:7,dtu:7,e:[7,9,10,11],each:[7,11],effect:11,effici:11,embed:[9,10,11],encod:[0,5,7,8,9,10],end:7,eta:2,even:2,everi:7,exactli:11,exist:7,exp:7,experi:[7,11],experiment:11,explain:11,explan:11,extend:7,extract:7,f:[2,7],f_1:7,f_2:7,f_a:2,featur:[2,7,11],few:[0,5,6],field:[0,5,6],figur:[7,11],fine:11,first:11,fisher:2,fix:11,focu:11,follow:11,foreground:2,formul:2,four:11,frac:11,framework:7,from:[0,2,5,6,11],fuse:11,g:[9,10,11],gamma:7,gener:2,give:2,given:[2,7],global:7,glue:11,gong19a:[],gong:11,guolin:[9,10,11],handl:7,hat:7,have:11,he:[3,9,10,11],head:11,henc:2,heterogen:[9,10,11],hopkin:[2,3],howev:11,html:[],http:[],i:[2,7,11],iclr:11,icml:11,idea:[],ij:11,illustr:11,imag:[0,2,5,6],implement:11,includ:[9,10,11],incorpor:7,increas:11,independ:7,indic:2,inform:[7,9,10,11],initi:7,innat:[0,4,5],input:[2,7],instead:11,int_:7,intermedi:7,interpol:7,introduc:2,invis:2,invol:7,involv:[9,10,11],its:7,j:3,john:[2,3],ju:3,just:11,k:11,kanazawa:7,ke:[9,10,11],kei:[],khandelw:11,kortylewski:[2,3],l:11,label:2,label_by_area:[],languag:[0,5,8],larg:2,latent:2,layer:[7,11],learn:[2,7,11],learnabl:11,learnt:2,left:7,let:[2,7],levi:11,li:11,like:11,link:[2,3,7,11],liu:[3,9,10,11],local:[7,11],look:11,m:[2,7],mai:11,make:11,man:11,mani:[7,11],map:2,margin:2,mathbb:[7,11],mathbf:7,mathcal:2,matric:11,matthew:7,method:[2,11],microsoft:[9,10,11],mid:2,mise:2,mix:[9,10,11],mixtur:2,mlr:[],model:[0,1,5,7],moreov:11,multipl:7,n:7,nearbi:[9,10,11],necessarili:11,nerf:[1,5,7],network:[0,2,4,5,7],neural:[0,2,4,5,6],non:2,note:[],novel:7,number:7,o:11,object:2,obtain:7,occlud:2,occlus:[0,2,4,5,7],often:11,one:7,onli:[2,11],onto:7,oper:7,optim:7,org:[],other:11,out:[0,4,5],outperfom:2,outperform:[2,7],overrightarrow:2,overview:7,p:[2,7],p_a:2,p_i:11,p_iw:11,p_j:11,p_jw:11,paper:11,paramet:11,partial:[0,4,5],pass:7,phi:7,pi:7,pipelin:7,pixelnerf:[0,5,6],plane:7,point:7,pool:7,posit:[0,5,8,9,10],pre:[0,5,8],predict:7,press:[],prior:2,problem:2,proceed:[],prod_:2,product:[9,10,11],progresss:11,project:[2,7,11],propos:[2,7,11],psi:2,pyramid:7,q:[3,11],qin:11,qing:3,quantit:7,r:[7,11],radianc:[0,5,6],rai:7,real:11,reconstruct:7,redund:11,regular:11,rel:11,relat:11,remov:11,render:7,represent:7,requir:7,research:[9,10,11],residu:7,resnet:7,resourc:[9,10,11],result:[7,11],rethink:[0,5,8],retriev:7,rgb:7,right:7,robust:[0,4,5],s:[7,11],same:[2,11],scene:7,score:11,second:11,seem:11,segment:[0,4,5],self:11,sentenc:11,set:11,sever:11,shape:2,shapenet:7,share:11,show:[7,11],sigma:7,significantli:7,singl:7,some:11,sort:[],space:7,spatial:2,sqrt:11,stack:11,stand:11,standard:7,state:7,strong:11,studi:11,subsequ:11,sum_:2,sun:2,supervis:2,synthesi:7,t:[7,11],t_f:7,t_n:7,tak:2,tancik:7,task:[0,4,5],technic:[],term:[9,10,11],test:7,text:11,textual:11,thecvf:3,therefor:11,theta:11,thi:[2,7,11],third:11,through:[0,4,5],thte:7,tie:[9,10,11],time:[2,7],token:[9,10,11],top:11,toward:[9,10,11],train:[0,2,5,8],transform:[1,5,7,9,10,11],treat:11,tupe:11,two:[9,10,11],u:11,uc:7,uniform:11,univers:[2,3],unti:11,unwant:[9,10,11],us:2,usual:[9,10,11],v97:[],v:7,valu:11,variabl:2,variant:11,vector:[7,11],veri:11,via:7,vicki:7,view:7,vision:[],visual:11,volum:7,volumetr:7,von:2,w:[2,7,11],w_a:2,w_i:11,w_iw:11,w_j:11,w_jw:11,wang:11,we:[7,9,10,11],weight:2,what:11,when:2,where:[2,7],which:[2,9,10,11],whole:11,wide:2,within:2,without:2,word:[9,10,11],work:[7,11],world:7,x:7,y:2,yan:[9,10,11],ye:7,yihong:2,yu:7,yuill:[2,3],z:11,zeta:2},titles:["Aug 2022","By Areas","Amodal Segmentation through Out-of-Task and Out-of-Distribution with a Bayesian Model","Compositional Convolutional Neural Networks: A Deep Architecture with Innate Robustness to Partial Occlusion","Compositional Models","Paper Reading Notes","NeRF","pixelNeRF: Neural Radiance Fields from One or Few Images","Transformers","Paper 1","Paper 2","Rethinking Positional Encoding in Language Pre-training"],titleterms:{"1":9,"2":10,"2022":0,A:3,By:1,One:7,amod:2,architectur:3,area:1,aug:0,bayesian:2,composit:[3,4],content:5,convolut:3,deep:3,detail:[2,3,7,11],distribut:2,encod:11,few:7,field:7,from:7,idea:[2,3,7,9,10,11],imag:7,innat:3,kei:[2,3,7,9,10,11],languag:11,model:[2,4],nerf:6,network:3,neural:[3,7],note:[2,3,5,7,11],occlus:3,out:2,paper:[4,5,6,8,9,10],partial:3,pixelnerf:7,posit:11,pre:11,radianc:7,read:5,refer:[2,3,7,11],rethink:11,robust:3,segment:2,summari:[2,3,7,11],task:2,technic:[2,3,7,11],through:2,train:11,transform:8,vision:[]}})